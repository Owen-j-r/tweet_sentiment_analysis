{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets from North America...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 181/5000 [00:09<03:34, 22.45it/s]/var/folders/ly/dg_yy0m54vb9my4bfgbtld880000gn/T/ipykernel_7156/2429112758.py:30: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tweet.content,\n",
      "12696it [12:59, 16.28it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 4999 tweets from North America\n",
      "Scraping tweets from Europe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20736it [17:16, 20.01it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 4999 tweets from Europe\n",
      "Scraping tweets from Oceania...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9501it [10:25, 15.18it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 4999 tweets from Oceania\n",
      "Scraping tweets from India & Pakistan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10063it [10:33, 15.87it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 4999 tweets from India & Pakistan\n",
      "Scraping tweets from Southern Africa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9237it [10:10, 15.13it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 4999 tweets from Southern Africa\n"
     ]
    }
   ],
   "source": [
    "# Define the search keyword and date range\n",
    "keyword = \"climate change\"\n",
    "since_date = \"2021-01-21\"\n",
    "until_date = \"2023-03-21\"\n",
    "tweet_limit = 5000  # Set to 1000 so that you get 1000 tweets from each region, totaling 3000 tweets\n",
    "\n",
    "# Define the geocodes for the regions\n",
    "geocodes = [\n",
    "    (\"North America\", \"39.8283,-98.5795,5000km\"),\n",
    "    (\"Europe\", \"54.5260,15.2551,3000km\"),\n",
    "    (\"Oceania\", \"-24.7736,133.5833,6000km\"),\n",
    "    (\"India & Pakistan\", \"24.0,75.0,2000km\"),\n",
    "    (\"Southern Africa\", \"-19.0154,29.1549,4000km\"),\n",
    "]\n",
    "\n",
    "tweets = []\n",
    "\n",
    "for region, geocode in geocodes:\n",
    "    print(f\"Scraping tweets from {region}...\")\n",
    "    #query = f'{keyword} since:{since_date} until:{until_date} geocode:{geocode}'\n",
    "    query = f'{keyword} geocode:{geocode}'\n",
    "    scraper = sntwitter.TwitterSearchScraper(query)\n",
    "    \n",
    "    count = 0\n",
    "    for i, tweet in tqdm(enumerate(scraper.get_items()), total=tweet_limit):\n",
    "        if tweet.coordinates:  # Check if geolocation data is available\n",
    "            data = [\n",
    "                tweet.date,\n",
    "                tweet.id,\n",
    "                tweet.content,\n",
    "                tweet.user.username,\n",
    "                tweet.coordinates,\n",
    "                tweet.likeCount,\n",
    "                tweet.retweetCount,\n",
    "                region\n",
    "            ]\n",
    "            tweets.append(data)\n",
    "            count += 1\n",
    "\n",
    "            if count >= tweet_limit - 1:\n",
    "                break\n",
    "            \n",
    "    print(f\"Collected {count} tweets from {region}\")\n",
    "    \n",
    "tweet_df = pd.DataFrame(\n",
    "    tweets, columns=['date', 'id', 'content', 'username', 'coordinates', 'like_count', 'retweet_count','region']\n",
    ")\n",
    "\n",
    "keyword = keyword.replace(' ', '_')\n",
    "tweet_df.to_csv(f'{keyword}-tweets.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "333d2c9757a75282c9d4867cccb346dd97fbdd6438562bec117fa31e687fe23c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
